services:
  # 맥북에서는 vLLM 대신 Ollama를 사용하여 API 테스트 (Spring Boot는 차이를 모름)
  llm-server:
    image: ollama/ollama:latest
    container_name: local-llm
    # Mac은 GPU 설정을 따로 하지 않아도 Docker가 CPU/메모리를 사용함
    # (Ollama Docker는 Mac GPU 가속을 완벽 지원하진 않지만, 테스트용으로 충분)
    ports:
      - "11434:11434"
    volumes:
      - ../../datas/ollama:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h # 테스트 중 모델이 메모리에서 내려가지 않게 설정
    restart: no